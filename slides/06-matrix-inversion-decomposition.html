<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Matrix inversion and decomposition</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACS 33000   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Matrix inversion and decomposition
### <a href="https://github.com/math-camp/course">MACS 33000</a> <br /> University of Chicago

---







# Learning objectives

* Define matrix inversion
* Demonstrate how to solve systems of linear equations using matrix inversion
* Define the determinant of a matrix
* Define matrix decomposition
* Explain singular value decomposition and demonstrate the applicability of matrix algebra to real-world problems

---

# Matrix inversion

* `\(\mathbf{X}\)` is an `\(n \times n\)` matrix
* Find the matrix `\(\mathbf{X}^{-1}\)` such that 

    `$$\mathbf{X}^{-1} \mathbf{X} = \mathbf{X} \mathbf{X}^{-1} = \mathbf{I}$$`

    where `\(\mathbf{I}\)` is the `\(n \times n\)` identity matrix

--

* Why?
    * Solve systems of equations
    * Perform linear regression
    * Provide intuition about colinearity, fixed effects, and other relevant design matricies

---

# Calculating matrix inversions

$$
`\begin{aligned}
x_{1} + x_{2} + x_{3} &amp;= 0 \\
0x_{1} 	+ 	5x_{2} + 0x_{3}  &amp; = 5 \\
0 x_{1} + 0 x_{2} + 3 x_{3} &amp; =  6 \\
\end{aligned}`
$$

--

$$
`\begin{aligned}
\mathbf{A}  &amp;= \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 5 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{bmatrix} \\
\mathbf{x} &amp;= (x_{1} , x_{2}, x_{3} ) \\
\mathbf{b} &amp;= (0, 5, 6)
\end{aligned}`
$$

* System of equations

    `$$\mathbf{A}\mathbf{x} =\mathbf{b}$$`

* `\(\mathbf{A}^{-1}\)` exists **if and only if** `\(\mathbf{A}\mathbf{x}  =  \mathbf{b}\)` has only one solution

---

# Definition

* Suppose `\(\mathbf{X}\)` is an `\(n \times n\)` matrix
* Call `\(\mathbf{X}^{-1}\)` the **inverse** of `\(\mathbf{X}\)` if

    `$$\mathbf{X}^{-1} \mathbf{X} = \mathbf{X} \mathbf{X}^{-1} = \mathbf{I}$$`

* If `\(\mathbf{X}^{-1}\)` exists then `\(\mathbf{X}\)` is invertible
* If `\(\mathbf{X}^{-1}\)` does not exist, `\(\mathbf{X}\)` is **singular**

---

# Inverting a matrix

* `\(\mathbf{A}^{-1}\)`
    

```
##      [,1] [,2]   [,3]
## [1,]    1 -0.2 -0.333
## [2,]    0  0.2  0.000
## [3,]    0  0.0  0.333
```

--

* `\(\mathbf{x}\)`
    

```
## [1] -3  1  2
```

---

# When do inverses exist

* Linear independence

--

### Example 1

`$$\mathbf{v}_{1} = (1, 0, 0), \quad \mathbf{v}_{2} = (0,1,0), \quad \mathbf{v}_{3} = (0,0,1)$$`

--

### Example 2

`$$\mathbf{v}_{1} = (1, 0, 0), \quad \mathbf{v}_{2} = (0,1,0), \quad \mathbf{v}_{3} = (0,0,1), \quad \mathbf{v}_{4} = (1, 2, 3)$$`

--

`$$\mathbf{v}_{4} = \mathbf{v}_{1} + 2 \mathbf{v}_{2} + 3\mathbf{v}_{3}$$`

---

# Inverting a `\(2 \times 2\)` matrix

* If `\(\mathbf{A} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)` and `\(ad \neq bc\)`, then `\(\mathbf{A}\)` is invertible and

    `$$\mathbf{A}^{-1} = \frac{1}{ad - bc} \begin{bmatrix}d &amp; -b \\-c &amp; a\end{bmatrix}$$`

---

# Inverting a `\(2 \times 2\)` matrix

`$$\mathbf{A} = \begin{bmatrix}9 &amp; 7 \\2 &amp; 1\end{bmatrix} \\\mathbf{A}^{-1} = \frac{1}{(-5)} \begin{bmatrix}1 &amp; -7 \\-2 &amp; 9\end{bmatrix} = \begin{bmatrix}-0.2 &amp; 1.4 \\0.4 &amp; -1.8\end{bmatrix}$$`

--

## Verification

`$$\mathbf{A}^{-1} \mathbf{A} = \begin{bmatrix}9 &amp; 7 \\2 &amp; 1\end{bmatrix} \begin{bmatrix}-0.2 &amp; 1.4 \\0.4 &amp; -1.8\end{bmatrix} = \begin{bmatrix}1 &amp; 0 \\0 &amp; 1\end{bmatrix} = \mathbf{I}$$`

---

# Inverting an `\(n \times n\)` matrix

* Gauss-Jordan elimination
* Augmented matrix
* Elementary row operations
    1. Exchanging two rows in the matrix
    1. Subtracting a multiple of one row from another row
* Obtain a diagonal matrix

---

# Example of Gauss-Jordan

`$$\mathbf{A} = \begin{bmatrix}
2 &amp; 1 &amp; 2 \\
3 &amp; 1 &amp; 1 \\
3 &amp; 1 &amp; 2
\end{bmatrix}$$`

##### Setup the augmented matrix

`$$\left[\begin{array}{rrr|rrr}2 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\3 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\3 &amp; 1 &amp; 2 &amp; 0 &amp; 0 &amp; 1\end{array}\right]$$`

##### Substract `\(3/2\)` times the first row from each of the other rows

`$$\left[\begin{array}{rrr|rrr}2 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\0 &amp; -1/2 &amp; -1 &amp; -3/2 &amp; 0 &amp; 1\end{array}\right]$$`

---

# Example of Gauss-Jordan

##### Substract `\(3/2\)` times the first row from each of the other rows

`$$\left[\begin{array}{rrr|rrr}2 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\0 &amp; -1/2 &amp; -1 &amp; -3/2 &amp; 0 &amp; 1\end{array}\right]$$`

##### Add twice the second row to the first row, and subtract the second row from the third row

`$$\left[\begin{array}{rrr|rrr}2 &amp; 0 &amp; -2 &amp; -2 &amp; 2 &amp; 0 \\0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1\end{array}\right]$$`

---

# Example of Gauss-Jordan

##### Add twice the second row to the first row, and subtract the second row from the third row

`$$\left[\begin{array}{rrr|rrr}2 &amp; 0 &amp; -2 &amp; -2 &amp; 2 &amp; 0 \\0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1\end{array}\right]$$`

##### Add twice the third row to the first and second rows

`$$\left[\begin{array}{rrr|rrr}2 &amp; 0 &amp; 0 &amp; -2 &amp; 0 &amp; 2 \\0 &amp; -1/2 &amp; 0 &amp; -3/2 &amp; -1 &amp; 2 \\0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1\end{array}\right]$$`
    
---

# Example of Gauss-Jordan

##### Add twice the third row to the first and second rows

`$$\left[\begin{array}{rrr|rrr}2 &amp; 0 &amp; 0 &amp; -2 &amp; 0 &amp; 2 \\0 &amp; -1/2 &amp; 0 &amp; -3/2 &amp; -1 &amp; 2 \\0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1\end{array}\right]$$`

##### Norm out the diagonal matrix to be `\(\mathbf{I}\)`
    
`$$\mathbf{A}^{-1} = \begin{bmatrix}-1 &amp; 0 &amp; 1 \\3 &amp; 2 &amp; -4 \\0 &amp; -1 &amp; 1\end{bmatrix}$$`

---

# Another example

`$$\mathbf{A} = \begin{bmatrix}
1 &amp; 3 &amp; 5 \\
1 &amp; 7 &amp; 5 \\
5 &amp; 10 &amp; 15
\end{bmatrix}$$`

##### First setup the augmented matrix

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\1 &amp; 7 &amp; 5 &amp; 0 &amp; 1 &amp; 0 \\5 &amp; 10 &amp; 15 &amp; 0 &amp; 0 &amp; 1\end{array}\right]$$`

##### Subtract 3 x row 1 from row 2

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\5 &amp; 10 &amp; 15 &amp; 0 &amp; 0 &amp; 1\end{array}\right]$$`
    
---

# Another example

##### Subtract 3 x row 1 from row 2

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\5 &amp; 10 &amp; 15 &amp; 0 &amp; 0 &amp; 1\end{array}\right]$$`
    
##### Subtract 2 x row 1 from row 3

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\0 &amp; -3 &amp; -3 &amp; -2 &amp; 0 &amp; 1\end{array}\right]$$`
    
---

# Another example

##### Subtract 2 x row 1 from row 3

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\0 &amp; -3 &amp; -3 &amp; -2 &amp; 0 &amp; 1\end{array}\right]$$`
    
##### Subtract `\(\frac{3}{4}\)` x row 2 from row 3

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 3 &amp; \frac{1}{4} &amp; -\frac{3}{4} &amp; 1\end{array}\right]$$`
    
---

# Another example

##### Subtract `\(\frac{3}{4}\)` x row 2 from row 3

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 3 &amp; \frac{1}{4} &amp; -\frac{3}{4} &amp; 1\end{array}\right]$$`
    
##### Divide row 3 by 3

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

---

# Another example

##### Divide row 3 by 3

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; -8 &amp; -3 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

##### Add 8 times row 3 to row 2

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; 0 &amp; -\frac{7}{3} &amp; -1 &amp; \frac{8}{3} \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

---

# Another example

##### Add 8 times row 3 to row 2

`$$\left[\begin{array}{rrr|rrr}1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; -4 &amp; 0 &amp; -\frac{7}{3} &amp; -1 &amp; \frac{8}{3} \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

##### Subtract 3 x row 3 from row 1

`$$\left[\begin{array}{rrr|rrr}1 &amp; 2 &amp; 0 &amp; \frac{3}{4} &amp; \frac{3}{4} &amp; -1 \\0 &amp; -4 &amp; 0 &amp; -\frac{7}{3} &amp; -1 &amp; \frac{8}{3} \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

---

# Another example

##### Subtract 3 x row 3 from row 1

`$$\left[\begin{array}{rrr|rrr}1 &amp; 2 &amp; 0 &amp; \frac{3}{4} &amp; \frac{3}{4} &amp; -1 \\0 &amp; -4 &amp; 0 &amp; -\frac{7}{3} &amp; -1 &amp; \frac{8}{3} \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

##### Divide row 2 by -4

`$$\left[\begin{array}{rrr|rrr}1 &amp; 2 &amp; 0 &amp; \frac{3}{4} &amp; \frac{3}{4} &amp; -1 \\0 &amp; 1 &amp; 0 &amp; \frac{7}{12} &amp; \frac{1}{4} &amp; -\frac{2}{3} \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

---

# Another example

##### Divide row 2 by -4

`$$\left[\begin{array}{rrr|rrr}1 &amp; 2 &amp; 0 &amp; \frac{3}{4} &amp; \frac{3}{4} &amp; -1 \\0 &amp; 1 &amp; 0 &amp; \frac{7}{12} &amp; \frac{1}{4} &amp; -\frac{2}{3} \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

##### Subtract 2 x row 2 from row 1

`$$\left[\begin{array}{rrr|rrr}1 &amp; 0 &amp; 0 &amp; -\frac{5}{12} &amp; \frac{1}{4} &amp; \frac{1}{3} \\0 &amp; 1 &amp; 0 &amp; \frac{7}{12} &amp; \frac{1}{4} &amp; -\frac{2}{3} \\0 &amp; 0 &amp; 1 &amp; \frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{array}\right]$$`

--

`$$\mathbf{A}^{-1} = \begin{bmatrix}-\frac{5}{12} &amp; \frac{1}{4} &amp; \frac{1}{3} \\\frac{7}{12} &amp; \frac{1}{4} &amp; -\frac{2}{3} \\\frac{1}{12} &amp; -\frac{1}{4} &amp; \frac{1}{3}\end{bmatrix} = \frac{1}{12}\begin{bmatrix}-5 &amp; 3 &amp; 4 \\7 &amp; 3 &amp; -8 \\1 &amp; -3 &amp; 4\end{bmatrix}$$`

---

# Regression analysis

$$
`\begin{aligned}
Y_{1} &amp; = \beta_{0} + \beta_{1} x_{11} + \beta_{2} x_{12} + \ldots + \beta_{k} x_{1k} \\
Y_{2} &amp; = \beta_{0} + \beta_{1} x_{21} + \beta_{2} x_{22} + \ldots + \beta_{k} x_{2k} \\
\vdots &amp; \vdots &amp; \vdots \\
Y_{i} &amp; = \beta_{0} + \beta_{1} x_{i1} + \beta_{2} x_{i2} + \ldots + \beta_{k} x_{ik} \\
\vdots &amp; \vdots &amp; \vdots \\
Y_{n} &amp; = \beta_{0} + \beta_{1} x_{n1} + \beta_{2} x_{n2} + \ldots + \beta_{k} x_{nk}
\end{aligned}`
$$

* `\(\mathbf{x}_{i} = (1, x_{i1}, x_{i2}, \ldots, x_{ik})\)`
* `\(\mathbf{X} = \begin{bmatrix} \mathbf{x}_{1}\\\mathbf{x}_{2}\\ \vdots \\ \mathbf{x}_{n} \end{bmatrix}\)`
* `\(\boldsymbol{\beta} = (\beta_{0}, \beta_{1}, \ldots, \beta_{k} )\)`
* `\(\mathbf{Y} = (Y_{1}, Y_{2}, \ldots, Y_{n})\)`

---

# Solve the system of equations

$$
`\begin{aligned}
\mathbf{Y} &amp;= \mathbf{X}\mathbf{\beta} \\
\mathbf{X}^{'} \mathbf{Y} &amp;= \mathbf{X}^{'} \mathbf{X} \mathbf{\beta} \\
(\mathbf{X}^{'}\mathbf{X})^{-1} \mathbf{X}^{'} \mathbf{Y} &amp;= (\mathbf{X}^{'}\mathbf{X})^{-1}\mathbf{X}^{'} \mathbf{X} \mathbf{\beta} \\
(\mathbf{X}^{'}\mathbf{X})^{-1} \mathbf{X}^{'} \mathbf{Y} &amp;=\mathbf{\beta} 
\end{aligned}`
$$

---

# Tax benefits of charitable contributions

* Company earns before-tax profits of $100,000
* 10% contribution of after-tax profits to charity
* State tax is 5% of profits (after charitable deduction)
* Federal tax is 40% of profits (after charitable and state tax deduction)
* How much does the company pay in state taxes, federal taxes, and Red Cross donation?

---

# Tax benefits of charitable contributions

$$
`\begin{aligned}
C &amp;+ 0.1S &amp;+ 0.1F &amp;= 10{,}000 \\
0.05C &amp;+ S &amp; &amp;= 5{,}000 \\
0.4C &amp;+ 0.4S &amp;+ F &amp;= 40{,}000
\end{aligned}`
$$

---

# Solve via matrix inversion

##### `\(\mathbf{A}\)`


```
##      [,1] [,2] [,3]
## [1,] 1.00  0.1  0.1
## [2,] 0.05  1.0  0.0
## [3,] 0.40  0.4  1.0
```

--

##### `\(\mathbf{b}\)`


```
## [1] 10000  5000 40000
```

--

##### `\(\mathbf{A}^{-1} \mathbf{b}\)`
    

```
## [1]  5956  4702 35737
```
    
---

# Determinant

* Single number summary of a square matrix
* Summary of structure
* Complicated to calculate

---

# `\(2 \times 2\)` matrix

`$$\det(\mathbf{X}) = \mid \mathbf{X} \mid = \left| \begin{matrix}
x_{11} &amp; x_{12} \\
x_{21} &amp; x_{22}
\end{matrix} \right| = x_{11}x_{22} - x_{12}x_{21}$$`

--

`$$\left| \begin{matrix}
1 &amp; 2 \\
3 &amp; 4
\end{matrix} \right| = (1)(4) - (2)(3) = 4 - 6 = -2$$`

`$$\left| \begin{matrix}
10 &amp; \frac{1}{2} \\
4 &amp; 1
\end{matrix} \right| = (10)(1) - \left( \frac{1}{2} \right)(4) = 10 - 2 = 8$$`

`$$\left| \begin{matrix}
2 &amp; 3 \\
6 &amp; 9
\end{matrix} \right| = (2)(9) - (3)(6) = 18 - 18 = 0$$`

---

# `\(n \times n\)` matrix

### Submatrix

`$$\mathbf{X} = \begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} &amp; x_{14} \\
x_{21} &amp; x_{22} &amp; x_{23} &amp; x_{24} \\
x_{31} &amp; x_{32} &amp; x_{33} &amp; x_{34} \\
x_{41} &amp; x_{42} &amp; x_{43} &amp; x_{44} \\
\end{bmatrix}$$`

`$$\mathbf{X}_{[11]} = \begin{bmatrix}
x_{22} &amp; x_{23} &amp; x_{24} \\
x_{32} &amp; x_{33} &amp; x_{34} \\
x_{42} &amp; x_{43} &amp; x_{44} \\
\end{bmatrix},
\mathbf{X}_{[24]} = \begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13}  \\
x_{31} &amp; x_{32} &amp; x_{33}  \\
x_{41} &amp; x_{42} &amp; x_{43}  \\
\end{bmatrix}$$`

---

# `\(n \times n\)` matrix

`$$\mid \mathbf{X} \mid = \sum_{j=1}^n (-1)^{i+j} x_{ij} \mid\mathbf{X}_{[ij]}\mid$$`

* `\(ij\)`th **minor** of `\(\mathbf{X}\)` for `\(x_{ij}\)`, `\(\mid\mathbf{X}_{[ij]}\mid\)`
    * Determinant of the `\((n - 1) \times (n - 1)\)` submatrix that results from taking the `\(i\)`th row and `\(j\)`th column out
* **Cofactor** of `\(\mathbf{X}\)`
    * Minor signed as `\((-1)^{i+j} x_{ij} \mid\mathbf{X}_{[ij]}\mid\)`
* Cycle recursively through columns

---

# `\(3 \times 3\)` matrix

$$
`\begin{aligned}
\mathbf{X} &amp;= \begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} \\
x_{21} &amp; x_{22} &amp; x_{23} \\
x_{31} &amp; x_{32} &amp; x_{33} \\
\end{bmatrix} \\
\det(\mathbf{X}) &amp;= (+1)x_{11} \left| \begin{matrix}
x_{22} &amp; x_{23} \\
x_{32} &amp; x_{33} \\
\end{matrix} \right| +(-1)x_{12} \left| \begin{matrix}
x_{21} &amp; x_{23} \\
x_{31} &amp; x_{33} \\
\end{matrix} \right| + (+1)x_{13} \left| \begin{matrix}
x_{21} &amp; x_{22} \\
x_{31} &amp; x_{32} \\
\end{matrix} \right|
\end{aligned}`
$$

---

# Why do we care?

`$$\mathbf{A}^{-1} = \frac{1}{ad - bc} \begin{bmatrix}
d &amp; -b \\
-c &amp; a
\end{bmatrix}$$`

---

# Matrix decomposition

* Factorization of a matrix into a product of matricies
* Decomposed into more efficient matricies depending on the calculations needing to be performed.

--

## LU decomposition

`$$\mathbf{A} = \mathbf{L}\mathbf{U}$$`

* Only for square matricies

---

# Dimension reduction

* Decreasing the number of dimensions in your dataset
* Rational
    1. Visualization
    1. Improving a statistical learning model
* Identify a smaller set of representative variables/vectors/columns that collectively explain most of the variability in the original dataset

---

# DW-NOMINATE

* Studying legislative voting
* Roll-call votes
* Multidimensional scaling techniques
* DW-NOMINATE dimensions
    * Ideology (liberal-conservative)
    * Issue of the day

---

# House: First dimension

&lt;img src="images/house_party_means_1879-2015.png" width="819" style="display: block; margin: auto;" /&gt;

---

# House: Second dimension

&lt;img src="images/house_party_means_1879-2015_2nd.png" width="671" style="display: block; margin: auto;" /&gt;

---

# Senate: First dimension

&lt;img src="images/senate_party_means_1879-2015.png" width="671" style="display: block; margin: auto;" /&gt;

---

# Senate: Second dimension

&lt;img src="images/senate_party_means_1879-2015_2nd.png" width="671" style="display: block; margin: auto;" /&gt;

---

# Singular value decomposition

* Matrix decomposition method
* Works with any rectangular matrix 
* `\(\mathbf{M}\)` is an `\(m \times n\)` matrix. There exists a factorization of the form

    `$$\mathbf{M} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{*}$$`

    where

    * `\(\mathbf{U}\)` is an `\(m \times n\)` matrix
    * `\(\boldsymbol{\Sigma}\)` is an `\(n \times n\)` diagonal matrix
    * `\(\mathbf{V}^{*}\)` is the transpose of an `\(n \times n\)` matrix

---

# Image compression



&lt;img src="06-matrix-inversion-decomposition_files/figure-html/read-image-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Image compression


```
##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] 0.361 0.369 0.381 0.393 0.403
## [2,] 0.365 0.373 0.385 0.397 0.407
## [3,] 0.369 0.377 0.389 0.399 0.411
## [4,] 0.377 0.385 0.395 0.407 0.420
## [5,] 0.388 0.391 0.403 0.416 0.424
```

---

# Applying SVD



##### `\(\mathbf{U}\)`


```
##         [,1]    [,2]     [,3]
## [1,] -0.0398 -0.0291 -0.02032
## [2,] -0.0405 -0.0150 -0.00198
## [3,] -0.0396 -0.0186 -0.01972
```
    
##### `\(\boldsymbol{\Sigma}\)`


```
##      [,1] [,2] [,3]
## [1,]  193  0.0  0.0
## [2,]    0 29.2  0.0
## [3,]    0  0.0 16.2
```

##### `\(\mathbf{V}^{*}\)`


```
##         [,1]    [,2]   [,3]
## [1,] -0.0556 0.00838 0.0211
## [2,] -0.0558 0.00848 0.0179
## [3,] -0.0560 0.00874 0.0138
```

---

# Recovering the data

* Multiply back together

    `$$\mathbf{M} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{*}$$`


```
##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] 0.361 0.369 0.381 0.393 0.403
## [2,] 0.365 0.373 0.385 0.397 0.407
## [3,] 0.369 0.377 0.389 0.399 0.411
## [4,] 0.377 0.385 0.395 0.407 0.420
## [5,] 0.388 0.391 0.403 0.416 0.424
```

---

# Data reduction

* `\(\Sigma\)`


```
##   [1] 193.4417  29.1733  16.1600  14.9806  12.1708  11.3756  10.5788
##   [8]   8.9693   8.3404   7.6359   7.4752   6.8798   6.1244   5.9575
##  [15]   5.5327   5.3978   5.1953   4.8511   4.6521   4.6020   4.2501
##  [22]   4.1820   4.0820   4.0382   3.8938   3.8375   3.7173   3.5563
##  [29]   3.5273   3.4986   3.4396   3.4027   3.3417   3.2681   3.2025
##  [36]   3.1409   3.0671   3.0221   3.0124   2.9543   2.8912   2.8365
##  [43]   2.8076   2.7306   2.6768   2.6547   2.6008   2.5562   2.5353
##  [50]   2.5186   2.4892   2.4669   2.3997   2.3361   2.3274   2.2823
##  [57]   2.2424   2.2378   2.1923   2.1692   2.1122   2.0840   2.0704
##  [64]   2.0510   2.0241   2.0196   1.9849   1.9568   1.9305   1.9237
##  [71]   1.9052   1.8737   1.8433   1.8222   1.8107   1.7891   1.7699
##  [78]   1.7554   1.7195   1.7039   1.6870   1.6695   1.6453   1.6310
##  [85]   1.6101   1.5815   1.5727   1.5373   1.5198   1.5105   1.4861
##  [92]   1.4748   1.4609   1.4378   1.4321   1.4016   1.4001   1.3788
##  [99]   1.3624   1.3386   1.3301   1.3169   1.3057   1.2704   1.2593
## [106]   1.2419   1.2376   1.2065   1.1922   1.1825   1.1741   1.1584
## [113]   1.1405   1.1314   1.1157   1.1003   1.0921   1.0705   1.0602
## [120]   1.0480   1.0406   1.0314   1.0191   0.9983   0.9939   0.9919
## [127]   0.9634   0.9500   0.9434   0.9337   0.9213   0.9153   0.9044
## [134]   0.8910   0.8777   0.8528   0.8458   0.8419   0.8246   0.8196
## [141]   0.8005   0.7967   0.7924   0.7866   0.7734   0.7591   0.7564
## [148]   0.7469   0.7365   0.7283   0.7198   0.7159   0.7118   0.7009
## [155]   0.6926   0.6874   0.6817   0.6634   0.6552   0.6517   0.6493
## [162]   0.6352   0.6184   0.6127   0.6073   0.6039   0.6014   0.5949
## [169]   0.5915   0.5810   0.5767   0.5627   0.5547   0.5456   0.5381
## [176]   0.5351   0.5310   0.5247   0.5211   0.5139   0.5025   0.4998
## [183]   0.4966   0.4808   0.4763   0.4725   0.4613   0.4552   0.4529
## [190]   0.4471   0.4411   0.4374   0.4326   0.4309   0.4232   0.4178
## [197]   0.4152   0.4047   0.4005   0.3970   0.3884   0.3795   0.3790
## [204]   0.3770   0.3705   0.3690   0.3597   0.3535   0.3506   0.3465
## [211]   0.3434   0.3387   0.3341   0.3243   0.3201   0.3183   0.3099
## [218]   0.3073   0.3020   0.2980   0.2972   0.2953   0.2911   0.2826
## [225]   0.2787   0.2738   0.2705   0.2644   0.2584   0.2542   0.2533
## [232]   0.2472   0.2424   0.2397   0.2356   0.2320   0.2300   0.2268
## [239]   0.2205   0.2187   0.2160   0.2096   0.2077   0.1980   0.1961
## [246]   0.1930   0.1895   0.1891   0.1853   0.1814   0.1798   0.1772
## [253]   0.1720   0.1704   0.1681   0.1658   0.1650   0.1617   0.1539
## [260]   0.1523   0.1483   0.1457   0.1436   0.1424   0.1367   0.1360
## [267]   0.1332   0.1304   0.1276   0.1265   0.1259   0.1232   0.1201
## [274]   0.1158   0.1119   0.1112   0.1079   0.1069   0.1044   0.1010
## [281]   0.0993   0.0980   0.0934   0.0905   0.0900   0.0878   0.0868
## [288]   0.0847   0.0838   0.0796   0.0763   0.0744   0.0733   0.0710
## [295]   0.0682   0.0674   0.0671   0.0637   0.0612   0.0595   0.0570
## [302]   0.0556   0.0537   0.0501   0.0485   0.0446   0.0435   0.0426
## [309]   0.0401   0.0361   0.0354   0.0336   0.0311   0.0295   0.0286
## [316]   0.0257   0.0248   0.0238   0.0235   0.0233   0.0224   0.0221
## [323]   0.0218   0.0208   0.0203   0.0200   0.0195   0.0191   0.0184
## [330]   0.0181   0.0175   0.0174   0.0170   0.0162   0.0157   0.0155
## [337]   0.0152
```

---

# Data reduction

&lt;img src="06-matrix-inversion-decomposition_files/figure-html/svd-sigma-prop-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Draw image using rank 2

&lt;img src="06-matrix-inversion-decomposition_files/figure-html/lions-rank1-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Different ranks

&lt;img src="06-matrix-inversion-decomposition_files/figure-html/lions-rank-all-1.gif" style="display: block; margin: auto;" /&gt;

---

# Principal components analysis

* Technique for dimension reduction
* Find a low-dimensional representation of the data that contains as much as possible of the variation
* Dimensions are linear combinations of the variables

---

# Implementing PCA

1. Rescale each column to be mean 0 and standard deviation 1
1. Compute the covariance matrix `\(\mathbf{S}\)`
    `$$\mathbf{S} = \dfrac{1}{N} \mathbf{X}' \mathbf{X}$$`
1. Compute the `\(K\)` largest **eigenvectors** of `\(\mathbf{S}\)`
    * Eigenvector - principal directions
    * Values of observations along eigenvector - principal components
    * Eigenvalues - amount of variance along the eigenvector

--

* Not an efficient approach
* Let's use SVD!

---

# Calculating the PCA

`$$\mathbf{M} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{*}$$`

* `\(\mathbf{V}^{*}\)` = principal directions (eigenvector)
* Columns of `\(\mathbf{U} \boldsymbol{\Sigma}\)` = principal components (values along eigenvector)
* Values of the diagonal elements of `\(\boldsymbol{\Sigma}\)` `\(\approx\)` eigenvalues

---

# `USArrests`











&lt;img src="06-matrix-inversion-decomposition_files/figure-html/usarrests-pc-scores-plot-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# `USArrests`

&lt;img src="06-matrix-inversion-decomposition_files/figure-html/usarrests-pc-scores-biplot-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# MNIST data set

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png" style="display: block; margin: auto;" /&gt;







---

# MNIST data set

&lt;img src="06-matrix-inversion-decomposition_files/figure-html/pixels-pca-pc12-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# MNIST data set

&lt;img src="06-matrix-inversion-decomposition_files/figure-html/pixels-pca-pc12-facet-1.png" width="864" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cfss.uchicago.edu/slides/macros.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script src="https://www.redditstatic.com/comment-embed.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
