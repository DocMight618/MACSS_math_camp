<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>General random variables</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACS 33000   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# General random variables
### <a href="https://github.com/math-camp/course">MACS 33000</a> <br /> University of Chicago

---







`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}}$$`

---

# Learning objectives

* Define a continuous random variable
* Identify continuous random variable distributions relevant to social science
* Define expected value and variance of continuous random variables
* Relate continuous random variables to discrete random variables
* Define cumulative distribution functions (CDFs) for continuous random variables and compare to discrete random variables
* Estimate probability of events using probability density functions (PDFs) and cumulative distribution functions (CDFs)

---

# Continuous random variables

* Random variables that are not discrete
    * Approval ratings
    * GDP
    * Wait time between wars: `\(X(t) = t\)` for all `\(t\)`
    * Proportion of vote received: `\(X(v) = v\)` for all `\(v\)`
* Many analogues to discrete probability distributions
* We need calculus to answer questions about probability

---

# Probability density function

&lt;img src="11-general-random-vars_files/figure-html/pdf-1.png" width="864" style="display: block; margin: auto;" /&gt;

--

What is the area under the curve under `\(f(x)\)` between `\(.5\)` and `\(2\)`?

--

`$$\int_{1/2}^{2} f(x)\,dx = F(2) - F(1/2)$$`

---

# Continuous random variable

`\(X\)` is a **continuous random variable** if there exists a nonnegative function defined for all `\(x \in \Re\)` having the property for any (measurable) set of real numbers `\(B\)`,

`$$\Pr(X \in B) = \int_{B} f_X(x)\,dx$$`

---

# Continuous random variable

The probability that the value of `\(X\)` falls within an interval is

`$$\Pr (a \leq X \leq b) = \int_a^b f_X(x) \,dx$$`

--

* For any single value `\(a\)`, `\(\Pr (X = a) = \int_a^a f_X(x) \,dx = 0\)`

--

### Requirements to be a PDF

* Non-negative
* Normalization property

    `$$\int_{-\infty}^{\infty} f_X(x) \,dx = \Pr (-\infty \leq X \leq \infty) = 1$$`

---

# Uniform random variable

`$$X \sim \text{Uniform}(0,1)$$`

`$$f_X(x) = \left\{    \begin{array}{ll}        c &amp; \quad \text{if } 0 \leq x \leq 1 \\        0 &amp; \quad \text{otherwise}    \end{array}\right.$$`

--

`$$1 = \int_{-\infty}^{\infty} f_X(x)\,dx = \int_0^1 c \,dx = c \int_0^1 \,dx = c$$`

---

# Uniform random variable

&lt;img src="11-general-random-vars_files/figure-html/unif-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Uniform random variable

$$
`\begin{aligned}
\Pr(X \in [0.2, 0.5]) &amp; = \int_{0.2}^{0.5} 1 \,dx \\
&amp; = X |^{0.5}_{0.2} \\
&amp; = 0.5  - 0.2  \\
&amp; = 0.3 
\end{aligned}`
$$

--

$$
`\begin{aligned}
\Pr(X \in [0.5, 0.5]) &amp; = \int_{0.5}^{0.5} 1\,dx \\
&amp; = X|^{0.5}_{0.5} \\
&amp; = 0.5 - 0.5 \\
&amp; = 0 
\end{aligned}`
$$

--

$$
`\begin{aligned}
\Pr(X \in \{[0, 0.2]\cup[0.5, 1]\}) &amp; = \int_{0}^{0.2} 1\,dx + \int_{0.5}^{1} 1\,dx \\
 &amp; = X_{0}^{0.2} + X_{0.5}^{1} \\
 &amp; = 0.2 - 0 + 1 - 0.5 \\
 &amp; = 0.7 
\end{aligned}`
$$

---

# Uniform random variable

`$$f_X(x) = \left\{    \begin{array}{ll}        \frac{1}{b-a} &amp; \quad \text{if } a \leq x \leq b \\        0 &amp; \quad \text{otherwise}    \end{array}\right.$$`

---

# Continuous random variables

* `\(\Pr(X = a) = 0\)`
* `\(\Pr(X \in (-\infty, \infty) ) = 1\)` 
* If `\(F\)` is antiderivative of `\(f\)`, then `\(\Pr(X \in [c,d]) = F(d) - F(c)\)`

---

# Expectation

`$$\E[X] = \int_{-\infty}^{\infty} x f_X(x) \,dx$$`

* Integration instead of summation

--

* Expected value rule

    `$$\E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) \,dx$$`

--

* `\(n\)`th moment = `\(\E[X^n]\)`
* `\(\Var(X) = (X - \E[X])^2 = \E[X^2] - (\E[X])^2\)`

---

# Uniform random variable

$$
`\begin{align}
\E[X] = \int_{-\infty}^{\infty} x f_X(x) \,dx &amp;= \int_a^b x \times \frac{1}{b-a} \,dx \\
&amp;= \frac{1}{b-a} \times \frac{1}{2}x^2 \Big|_a^b \\
&amp;= \frac{1}{b-a} \times \frac{b^2 - a^2}{2} \\
&amp;= \frac{a+b}{2}
\end{align}`
$$

--

$$
`\begin{align}
\E[X^2] = \int_a^b x^2 \times \frac{1}{b-a} \,dx &amp;= \frac{1}{b-a} \int_a^b x^2 \,dx \\
&amp;= \frac{1}{b-a} \times \frac{1}{3}x^3 \Big|_a^b \\
&amp;= \frac{b^3 - a^3}{3(b-a)} \\
&amp;= \frac{a^2 + ab + b^2}{3}
\end{align}`
$$

--

`$$\Var(X) = \E[X^2] - (\E[X])^2 = \frac{a^2 + ab + b^2}{3} - \left( \frac{a+b}{2} \right)^2 = \frac{(b-a)^2}{12}$$`

---

# Exponential random variable

`$$f_X(x) = \left\{    \begin{array}{ll}        \lambda e^{-\lambda x} &amp; \quad \text{if } x \geq 0 \\        0 &amp; \quad \text{otherwise}    \end{array}\right.$$`

* `\(\lambda &gt; 0\)`

---

# Exponential random variable

&lt;img src="11-general-random-vars_files/figure-html/exp-rv-1.png" width="864" style="display: block; margin: auto;" /&gt;

--

`$$\E[X] = \frac{1}{\lambda}, \quad \Var(X) = \frac{1}{\lambda^2}$$`

---

# Cumulative distribution function

For a continuous random variable `\(X\)` define its **cumulative distribution function** (CDF) `\(F_X(x)\)` as, 

`$$F_X(x) = \Pr(X \leq x) = \int_{-\infty} ^{x} f_X(t) \,dt$$`

---

# Uniform distribution

Suppose `\(X \sim \text{Uniform}(0,1)\)`

`$$\begin{aligned} F_X(x) &amp; = \Pr(X\leq x)  \\&amp; = 0 \text{, if }x&lt; 0 \\ &amp; = 1 \text{, if }x &gt;1 \\ &amp; = x \text{, if } x \in [0,1]\end{aligned}$$`

--

&lt;img src="11-general-random-vars_files/figure-html/unif-rv-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Properties of CDFs

* `\(F_X\)` is monotonically nondecreasing
* `\(F_X(x)\)` tends to `\(0\)` as `\(x \rightarrow -\infty\)`, and to `\(1\)` as `\(x \rightarrow \infty\)`
* `\(F_X(x)\)` is a continuous function of `\(x\)`
* If `\(X\)` is continuous, the PDF and CDF can be obtained from each other by integration or differentiation

    `$$F_X(x) = \int_{-\infty}^x f_X(t) \,dt, \quad f_X(x) = \frac{dF_X}{dx} (x)$$`

---

# Normal distribution

Suppose `\(X\)` is a random variable with `\(X \in \Re\)`

`$$f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$$`

--

`$$X \sim \text{Normal}(\mu, \sigma^2)$$`

---

# Normal distribution

&lt;img src="11-general-random-vars_files/figure-html/norm-rv-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Expected value/variance

* `\(Z\)` is a standard normal distribution if

    `$$Z \sim \text{Normal}(0,1)$$`

* CDF of `\(Z\)`

    `$$F_{Z}(x) = \frac{1}{\sqrt{2\pi} }\int_{-\infty}^{x} \exp(-z^2/2) \,dz$$`

---

# Expected value/variance

Suppose `\(Z \sim \text{Normal}(0,1)\)`

* `\(Y = 2Z + 6\)`
* `\(Y \sim \text{Normal}(6, 4)\)`

    &lt;img src="11-general-random-vars_files/figure-html/z-norm-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Expected value/variance

If `\(Z \sim N(0,1)\)`, then `\(Y = aZ + b\)` is

`$$Y \sim \text{Normal} (b, a^2)$$`

--

Assume we know

$$
`\begin{aligned}
\E[Z]  &amp; = 0 \\
\Var(Z) &amp; = 1 
\end{aligned}`
$$

--

For `\(Y \sim \text{Normal}(\mu, \sigma^2)\)`

`$$\begin{aligned} \E[Y] &amp; = \E[\sigma Z + \mu] \\&amp; = \sigma \E[Z] + \mu \\&amp; = \mu \\\Var(Y) &amp; = \Var(\sigma Z + \mu) \\ &amp; = \sigma^2 \Var(Z) + \Var(\mu) \\ &amp; = \sigma^2 + 0 \\ &amp; = \sigma^2 \end{aligned}$$`

---

# Standard normal distribution

If `\(X\)` is a normal random variable with mean `\(\mu\)` and variance `\(\sigma^2\)`, and if `\(a \neq 0, b\)` are scalars, then the random variable

`$$Y = aX + b$$`

is also normal, with mean and variance

`$$\E[Y] = a\mu + b, \quad \Var(Y) = a^2 \sigma^2$$`

--

### Why rely on the standard normal distribution

* Normal distribution is commonly used in statistical analysis
* Ease of standardization
* Saves time on the calculus

---

# Support for President Trump

Suppose we are interested in modeling presidential approval

* `\(Y\)`: proportion of population who "approves job president is doing"
* Individual responses are independent and identically distributed
* Average of those individual binary responses
* `\(N\rightarrow \infty\)`
* By Central Limit Theorm, `\(Y\)` is normally distributed

$$
`\begin{aligned}
Y &amp; \sim \text{Normal}(\mu, \sigma^2) \\
f_Y(y) &amp; = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y-\mu)^2}{2\sigma^2} \right)
\end{aligned}`
$$

---

# Central limit theorem

&lt;img src="11-general-random-vars_files/figure-html/clt-1.gif" style="display: block; margin: auto;" /&gt;

---

# Back to Trump

* Suppose `\(\mu = 0.39\)` and `\(\sigma^2 = 0.0025\)`
* `\(\Pr(Y\geq 0.45)\)` (What is the probability it isn't that bad?)

--

`$$\begin{aligned} \Pr(Y \geq 0.45)  &amp; = 1 - \Pr(Y \leq 0.45 ) \\&amp; = 1 - \Pr(0.05 Z + 0.39 \leq  0.45) \\&amp; = 1 - \Pr(Z \leq \frac{0.45-0.39 }{0.05} ) \\&amp; = 1 - \frac{1}{\sqrt{2\pi} } \int_{-\infty}^{6/5} \exp(-z^2/2) \,dz \\&amp; = 1 - F_{Z} (\frac{6}{5} ) \\&amp; = 0.1150697\end{aligned}$$`

---

# Gamma function

Suppose `\(\alpha&gt;0\)`. Define `\(\Gamma(\alpha)\)` as

$$
`\begin{aligned}
\Gamma(\alpha) &amp;= \int_{0}^{\infty} y^{\alpha- 1} e^{-y} \,dy \\
&amp;= (\alpha- 1)! \, \forall \alpha \in \{1, 2, 3, \ldots\}
\end{aligned}`
$$

--

* `\(\Gamma(\frac{1}{2}) = \sqrt{\pi}\)`

---

# Gamma function

Suppose we have `\(\Gamma(\alpha)\)`

$$
`\begin{aligned}
\frac{\Gamma(\alpha)}{\Gamma(\alpha)} &amp; = \frac{\int_{0}^{\infty} y^{\alpha-1} e^{-y} dy}{\Gamma(\alpha)} \\
1 &amp; = \int_{0}^{\infty} \frac{1}{\Gamma(\alpha)} y^{\alpha-1} e^{-y} \,dy 
\end{aligned}`
$$

--

Set `\(X = Y/\beta\)`

--

$$
`\begin{aligned}
F(x) = \Pr(X \leq x) &amp; = \Pr(Y/\beta \leq x ) \\
&amp; = \Pr(Y \leq x \beta ) \\
&amp; = F_{Y} (x \beta) \\
\frac{\partial F_{Y} (x \beta) }{\partial x} &amp; = f_{Y} (x \beta) \beta
\end{aligned}`
$$

--

`$$f(x|\alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-x\beta}$$`

---

# Gamma distribution

Suppose `\(X\)` is a continuous random variable, with `\(X \geq 0\)`. `\(X\)` is a Gamma random variable if

`$$f(x|\alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-x\beta}$$`

if `\(x\geq 0\)` and `\(0\)` otherwise

--

`$$X \sim \text{Gamma}(\alpha, \beta)$$`

---

# Gamma distribution

Suppose `\(X \sim \text{Gamma}(\alpha, \beta)\)`

$$
`\begin{aligned}
\E[X] &amp; = \frac{\alpha}{\beta} \\
\Var(X) &amp; = \frac{\alpha}{\beta^2}
\end{aligned}`
$$

--

Suppose `\(\alpha = 1\)` and `\(\beta = \lambda\)`. If

$$
`\begin{aligned}
X &amp; \sim \text{Gamma}(1, \lambda) \\
f(x|1, \lambda ) &amp; = \lambda e^{- x \lambda}
\end{aligned}`
$$

`$$X \sim \text{Exponential}(\lambda)$$`

---

# Properties of Gamma distributions

Suppose we have a sequence of independent random variables, with

`$$X_{i} \sim \text{Gamma}(\alpha_{i}, \beta)$$`

--

Then

`$$Y = \sum_{i=1}^{N} X_{i}$$`

`$$Y \sim \text{Gamma}(\sum_{i=1}^{N} \alpha_{i} , \beta)$$`

---

# Gamma distribution

&lt;img src="11-general-random-vars_files/figure-html/gamma-3-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Gamma distribution

&lt;img src="11-general-random-vars_files/figure-html/gamma-5-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Importance of the Gamma distribution

* Exponential and `\(\chi^2\)` distributions are special cases of the gamma distribution
* Commonly used in Bayesian statistics (conjugate prior)

---

# `\(\chi^2\)` distribution

Suppose `\(Z \sim \text{Normal}(0,1)\)`. Consider `\(X = Z^2\)`

$$
`\begin{aligned}
F_{X}(x)   &amp; =  \Pr(X \leq x) \\
&amp; = \Pr(Z^2 \leq x ) \\
&amp; = \Pr(-\sqrt{x} \leq Z \leq \sqrt{x}) \\
&amp; = \frac{1}{\sqrt{2\pi}} \int_{-\sqrt{x}}^{\sqrt{x} } e^{-\frac{z^2}{2}} \,dz\\
&amp; = F_{Z} (\sqrt{x}) - F_{Z} (-\sqrt{x})
\end{aligned}`
$$

--

$$
`\begin{aligned}
\frac{\partial F_{X}(x) }{\partial x }  &amp; = f_{Z} (\sqrt{x}) \frac{1}{2\sqrt{x}} + f_{Z}(-\sqrt{x}) \frac{1}{2\sqrt{x}} \\
&amp; = \frac{1}{\sqrt{x}}\frac{1}{2 \sqrt{2\pi}} ( 2e^{-\frac{x}{2}}) \\
&amp; = \frac{1}{\sqrt{x}}\frac{1}{\sqrt{2\pi}} ( e^{-\frac{x}{2}}) \\
&amp; = \frac{(\frac{1}{2})^{1/2}}{\Gamma(\frac{1}{2})}\left(x^{1/2 - 1} e^{-\frac{x}{2}}\right)
\end{aligned}`
$$

---

# `\(\chi^2\)` distribution

`$$f_X(x) = \frac{(\frac{1}{2})^{1/2}}{\Gamma(\frac{1}{2})}\left(x^{1/2 - 1} e^{-\frac{x}{2}}\right)$$`

--

`$$X \sim \text{Gamma}(1/2, 1/2)$$`

--

If `\(X = \sum_{i=1}^{N} Z^2\)`, `\(X \sim \text{Gamma}(n/2, 1/2)\)`

---

# `\(\chi^2\)` distribution

Suppose `\(X\)` is a continuous random variable with `\(X\geq 0\)`, with PDF 

`$$f(x) = \frac{1}{2^{n/2} \Gamma(n/2) } x^{n/2 - 1} e^{-x/2}$$`

`\(X\)` is a `\(\chi^2\)` distribution with `\(n\)` degrees of freedom

--

`$$X \sim \chi^{2}(n)$$`

---

# `\(\chi^2\)` distribution

&lt;img src="11-general-random-vars_files/figure-html/chi-sq-1.gif" style="display: block; margin: auto;" /&gt;

---

# `\(\chi^2\)` properties

Suppose `\(X \sim \chi^2(n)\)`

$$
`\begin{aligned}
\E[X] &amp; = \E\left[\sum_{i=1}^{N} Z_{i}^2\right] \\
 &amp; = \sum_{i=1}^{N} \E[Z_{i}^{2} ] \\
\Var(Z_{i} ) &amp; = \E[Z_{i}^2] - \E[Z_{i}]^2\\
1 &amp; = \E[Z_{i}^2]- 0 \\
\E[X] &amp; = n 
\end{aligned}`
$$

--

$$
`\begin{aligned}
\Var(X) &amp; = \sum_{i=1}^{N} \Var(Z_{i}^2) \\
&amp; = \sum_{i=1}^{N} \left(\E[Z_{i}^{4} ] - \E[Z_{i}]^{2}  \right) \\
&amp; = \sum_{i=1}^{N} \left(3 - 1\right ) = 2n 
\end{aligned}`
$$

---

# Student's `\(t\)` distribution

Suppose `\(Z \sim \text{Normal}(0, 1)\)` and `\(U \sim \chi^2(n)\)`.  Define the random variable `\(Y\)` as, 

`$$Y = \frac{Z}{\sqrt{\frac{U}{n}}}$$`

--

If `\(Z\)` and `\(U\)` are independent then `\(Y \sim t(n)\)`, with PDF 

`$$t(n) = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi n } \Gamma(\frac{n}{2})}\left(1 + \frac{x^2}{n}\right)^{-\frac{n+1}{2}}$$`

---

# Differences from the Normal distribution

* Normal distribution always has the same shape
* The shape of the student's `\(t\)`-distribution changes depending on the sample size
* Low `\(n\)`
* As `\(n \uparrow\)`, the confidence bounds shrink
* As `\(n \rightarrow \infty\)`, student's `\(t\)`-distribution takes on the same shape as the normal distribution

---

# Differences from the Normal distribution

&lt;img src="11-general-random-vars_files/figure-html/t-dist-1.gif" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cfss.uchicago.edu/slides/macros.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script src="https://www.redditstatic.com/comment-embed.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
