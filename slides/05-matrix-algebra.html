<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Matrix algebra</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACS 33000   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Matrix algebra
### <a href="https://github.com/math-camp/course">MACS 33000</a> <br /> University of Chicago

---







# Learning objectives

* Define vector and matrix
* Visualize vectors in multiple dimensions
* Demonstrate applicability of linear algebra to text analysis and cosine similarity
* Perform basic algebraic operations on vectors and matricies
* Generalize linear algebra to tensors and neural networks

---

# Linear algebra

* Data stored in **matricies**
* Higher dimensional spaces
    * Flood of big data, stored in many dimensions
* Linear algebra
    * Algebra of matricies
    * Geometry of high dimensional space
    * Calculus (multivariable) in many dimensions
* Very important for regression/machine learning/deep learning

---

# Points and vectors

* A point in `\(\Re^1\)`
    * `\(1\)`
    * `\(\pi\)`
    * `\(e\)`

--
* An ordered pair in `\(\Re^2 = \Re \times \Re\)`
    * `\((1,2)\)`
    * `\((0,0)\)`
    * `\((\pi, e)\)`

--
* An ordered triple in `\(\Re^3 = \Re \times \Re \times \Re\)`
    * `\((3.1, 4.5, 6.1132)\)`

--
* An ordered `\(n\)`-tuple in `\(R^n = \Re \times \Re \times \ldots \times \Re\)`
    * `\((a_{1}, a_{2}, \ldots, a_{n})\)`

---

# One dimensional example

&lt;img src="05-matrix-algebra_files/figure-html/one-d-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Two dimensional example

&lt;img src="05-matrix-algebra_files/figure-html/two-d-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Two dimensional example

&lt;img src="05-matrix-algebra_files/figure-html/two-d2-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Two dimensional example

&lt;img src="05-matrix-algebra_files/figure-html/two-d3-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Three dimensional example

* (Latitude, Longitude, Elevation)
* `\((1,2,3)\)`
* `\((0,1,2)\)`

--

## `\(N\)`-dimensional example

* Individual campaign donation records
    `$$\mathbf{x} = (1000, 0, 10, 50, 15, 4, 0, 0, 0, \ldots, 2400000000)$$`
* Counties have proportion of vote for Trump
    `$$\mathbf{y} = (0.8, 0.5, 0.6, \ldots, 0.2)$$`
* Run experiment, assess feeling thermometer of elected official
    `$$\mathbf{t} = (0, 100, 50, 70, 80, \ldots, 100)$$` 

---

# Vector/scalar addition/multiplication

$$
`\begin{aligned}
\mathbf{u} &amp; =  (1, 2, 3, 4, 5)  \\
\mathbf{v} &amp; =  (1, 1, 1, 1, 1)  \\
k &amp; =  2
\end{aligned}`
$$

--

$$
`\begin{aligned}
\mathbf{u}  + \mathbf{v} &amp; = (1 + 1, 2 + 1, 3+ 1, 4 + 1, 5+ 1)  = (2, 3, 4, 5, 6) \\
k \mathbf{u} &amp; = (2 \times 1, 2 \times 2, 2 \times 3, 2 \times 4, 2 \times 5) = (2, 4, 6, 8, 10)  \\
k \mathbf{v} &amp; = (2 \times 1,2 \times 1,2 \times 1,2 \times 1,2 \times 1) = (2, 2, 2, 2, 2)
\end{aligned}`
$$

---

# Linear dependence

* Linear combinations of vectors `\(\mathbf{a}\)` and `\(\mathbf{b}\)`

    `$$\mathbf{a} + \mathbf{b}$$`

    `$$2\mathbf{a} - 3\mathbf{b}$$`

* Generic form

    `$$\alpha \mathbf{a} + \beta\mathbf{b} + \gamma\mathbf{c} + \delta\mathbf{d} + \ldots$$`

* Linear independence

---

# Linear dependence

`$$\mathbf{a} = \begin{bmatrix}
3 \\
1
\end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix}
2 \\
2
\end{bmatrix}, \quad \mathbf{c} = \begin{bmatrix}
1 \\
3
\end{bmatrix}$$`

---

# Detecting linear dependence

* `\(\mathbf{b}^1, \mathbf{b}^2, \ldots \mathbf{b}^k\)` are linearly dependent if and only if there exist scalars `\(\alpha_1, \alpha_2, \ldots, \alpha_k\)` *not all zero* such that

    `$$\alpha_1 \mathbf{b}^1 + \alpha_2 \mathbf{b}^2 + \ldots + \alpha_k \mathbf{b}^k = \mathbf{0}$$`

---

# Example

`$$\mathbf{a} = \begin{bmatrix}
2 \\
1 \\
2
\end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix}
4 \\
1 \\
3
\end{bmatrix}, \quad \mathbf{c} = \begin{bmatrix}
1 \\
1 \\
2
\end{bmatrix}$$`

* Express as a **system of equations**

    `$$\begin{aligned}2\alpha &amp;+ 4\beta &amp;+ \gamma &amp;= 0 \\\alpha &amp;+ \beta &amp;+ \gamma &amp;= 0 \\2\alpha &amp;+ 3\beta &amp;+ 2\gamma &amp;= 0\end{aligned}$$`

---

# Solve the system of equations

`$$\begin{aligned}2\alpha &amp;+ 4\beta &amp;+ \gamma &amp;= 0 \\\alpha &amp;+ \beta &amp;+ \gamma &amp;= 0 \\2\alpha &amp;+ 3\beta &amp;+ 2\gamma &amp;= 0\end{aligned}$$`

--

$$
`\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ \gamma &amp;= 0 \\
 &amp;- \beta &amp;+ \frac{1}{2}\gamma &amp;= 0 \\
 &amp;- \beta &amp;+ \gamma &amp;= 0
\end{aligned}`
$$

--

$$
`\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ \gamma &amp;= 0 \\
 &amp;- \beta &amp;+ \frac{1}{2}\gamma &amp;= 0 \\
 &amp; &amp;+ \frac{1}{2} \gamma &amp;= 0
\end{aligned}`
$$

---

# Example

`$$\mathbf{a} = \begin{bmatrix}
2 \\
1 \\
2
\end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix}
4 \\
1 \\
3
\end{bmatrix}, \quad \mathbf{c} = \begin{bmatrix}
2 \\
2 \\
3
\end{bmatrix}$$`

---

# Example

$$
`\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ 2\gamma &amp;= 0 \\
\alpha &amp;+ \beta &amp;+ 2\gamma &amp;= 0 \\
2\alpha &amp;+ 3\beta &amp;+ 3\gamma &amp;= 0
\end{aligned}`
$$

--

$$
`\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ 2\gamma &amp;= 0 \\
 &amp;- \beta &amp;+ \gamma &amp;= 0 \\
 &amp;- \beta &amp;+ \gamma &amp;= 0
\end{aligned}`
$$

--

$$
`\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ 2\gamma &amp;= 0 \\
 &amp;- \beta &amp;+ \gamma &amp;= 0
\end{aligned}`
$$

--

`$$\gamma = 1, \quad \beta = 1, \quad \alpha = -3$$`

---

# Inner product

$$
`\begin{aligned}
\mathbf{u} \cdot \mathbf{v} &amp;=  u_{1} v_{1} + u_{2}v_{2} + \ldots + u_{n} v_{n}  \\
														&amp; =  \sum_{i=1}^{N} u_{i} v_{i} 
\end{aligned}`
$$

* Aka **dot product**
* Results in a scalar value

---

# Inner product

* `\(\mathbf{u} = (1, 2, 3)\)` and `\(\mathbf{v} = (2, 3, 1)\)`

`$$\begin{aligned} \mathbf{u} \cdot \mathbf{v} &amp; =  1 \times 2 +  2 \times 3 +  3 \times 1 \\ 				&amp; = 2+ 6 + 3 \\				&amp; = 11				\end{aligned}$$`

---

# Calculating vector length

&lt;img src="05-matrix-algebra_files/figure-html/pythagorean-theorem-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Vector norm

$$
`\begin{aligned}
\| \mathbf{v}\| &amp; = (\mathbf{v} \cdot \mathbf{v} )^{1/2} \\
						   &amp; = (v_{1}^2 + v_{2}^{2} + v_{3}^{2} + \ldots + v_{n}^{2} )^{1/2}
\end{aligned}`
$$

--

* Vector norm of a three-dimensional vector `\(\mathbf{x} = (1,1,1)\)`:

    `$$\begin{aligned}\| \mathbf{x}\| &amp; = (\mathbf{x} \cdot \mathbf{x} )^{1/2} \\						   &amp; = (x_{1}^2 + x_{2}^{2} + x_{3}^{2})^{1/2} \\						   &amp; = (1 + 1 + 1)^{1/2} \\						   &amp;= \sqrt{3}\end{aligned}$$`

---

# Text analysis

```
 a abandoned abc ability able about above abroad absorbed absorbing abstract
43         0   0       0    0    10     0      0        0         0        1
```

--

`$$(43,0,0,0,0,10,\dots)$$`

---

# Text analysis

$$
`\begin{aligned}
\text{Doc1} &amp; = (1, 1, 3, \ldots, 5) \\
\text{Doc2} &amp; = (2, 0, 0, \ldots, 1) \\
\textbf{Doc1}, \textbf{Doc2} &amp; \in \Re^{M}
\end{aligned}`
$$

---

# Inner product

$$
`\begin{aligned}
\textbf{Doc1} \cdot \textbf{Doc2}  &amp;  =  (1, 1, 3, \ldots, 5) (2, 0, 0, \ldots, 1)'  \\
 &amp; =  1 \times 2 + 1 \times 0 + 3 \times 0 + \ldots + 5 \times 1 \\
&amp; = 7 
\end{aligned}`
$$

---

# Length

$$
`\begin{aligned}
\| \textbf{Doc1} \| &amp; \equiv  \sqrt{ \textbf{Doc1} \cdot \textbf{Doc1} } \\
 &amp; =  \sqrt{(1, 1, 3, \ldots , 5) (1, 1, 3, \ldots, 5)' } \\
  &amp; =  \sqrt{1^{2} +1^{2} + 3^{2} + 5^{2} } \\
   &amp; =   6
\end{aligned}`
$$

---

# Cosine similarity

$$
`\begin{aligned}
\cos (\theta) &amp; \equiv  \left(\frac{\textbf{Doc1} \cdot \textbf{Doc2}}{\| \textbf{Doc1}\| \|\textbf{Doc2} \|} \right) \\
 &amp; = \frac{7} { 6 \times  2.24} \\
  &amp; = 0.52
\end{aligned}`
$$

---

# Measuring similarity

* Usefulness
* Desirable properties
    * The **maximum** should be the document with itself
    * The **minimum** should be documents which have no words in common
    * Increasing when more of the same words are used
    * Normalize for document length

---

# Using the inner product

`$$(2,1) \cdot (1,4) = 6$$`

&lt;img src="05-matrix-algebra_files/figure-html/inner-product-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Length dependence

`$$(4,2) \cdot (1,4) = 12$$`

&lt;img src="05-matrix-algebra_files/figure-html/inner-product-not-same-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Cosine similarity

&lt;img src="05-matrix-algebra_files/figure-html/cosine-sim-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

# Cosine similarity

$$
`\begin{aligned}
(4,2) \cdot (1,4) &amp;= 12 \\
\mathbf{a} \cdot \mathbf{b} &amp;= \|\mathbf{a} \| \times \|\mathbf{b} \| \times \cos(\theta) \\
\frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a} \| \times \|\mathbf{b} \|}  &amp;= \cos(\theta)
\end{aligned}`
$$

---

# Cosine similarity

$$
`\begin{aligned}
\cos (\theta) &amp; \equiv  \left(\frac{\textbf{Doc1} \cdot \textbf{Doc2}}{\| \textbf{Doc1}\| \|\textbf{Doc2} \|} \right) \\
 &amp; = \frac{(2, 1) \cdot (1, 4)} {\| (2,1)\| \| (1,4) \|} \\
 &amp; = \frac{6} {(\sqrt{2^2 + 1^2}) (\sqrt{1^2 + 4^2})} \\
 &amp; = \frac{6} {(\sqrt{5}) (\sqrt{17})} \\
  &amp; \approx 0.65
\end{aligned}`
$$

---

# Cosine similarity

$$
`\begin{aligned}
\cos (\theta) &amp; \equiv  \left(\frac{\textbf{Doc3} \cdot \textbf{Doc2}}{\| \textbf{Doc3}\| \|\textbf{Doc2} \|} \right) \\
 &amp; = \frac{(4,2) \cdot (1, 4)} {\| (24,2)\| \| (1,4) \|} \\
 &amp; = \frac{12} {(\sqrt{4^2 + 2^2}) (\sqrt{1^2 + 4^2})} \\
 &amp; = \frac{12} {(\sqrt{20}) (\sqrt{17})} \\
  &amp; \approx 0.65
\end{aligned}`
$$

---

# Matricies

* Rectangular arrangement (array) of numbers defined by two **axes**
    1. Rows
    1. Columns

`$$\mathbf{A} = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \ldots &amp; a_{mn} \\
\end{bmatrix}$$`

---

# Example matricies

`$$\begin{aligned}\mathbf{X} &amp;= \left[ \begin{array}{rrr}1 &amp; 2 &amp; 3 \\
2 &amp; 1 &amp; 4 \\
\end{array} \right] \\
\mathbf{Y} &amp;= \left[ \begin{array}{rr}
1 &amp; 2 \\
3 &amp; 2 \\
1 &amp; 4 \\
\end{array} \right]
\end{aligned}$$`

---

# Matrix addition

* `\(\mathbf{X}\)` and `\(\mathbf{Y}\)` are `\(m \times n\)` matrices

`$$\begin{aligned} 
\mathbf{X} + \mathbf{Y} &amp; =  \begin{pmatrix} 
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1n} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{m1} &amp; x_{m2} &amp; \ldots &amp; x_{mn} \\
\end{pmatrix} + 
\begin{pmatrix} 
y_{11} &amp; y_{12} &amp; \ldots &amp; y_{1n} \\
y_{21} &amp; y_{22} &amp; \ldots &amp; y_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
y_{m1} &amp; y_{m2} &amp; \ldots &amp; y_{mn} \\
\end{pmatrix} \\
&amp; = \begin{pmatrix} 
x_{11} + y_{11} &amp; x_{12} + y_{12} &amp; \ldots &amp; x_{1n} + y_{1n} \\
x_{21} + y_{21} &amp; x_{22} + y_{22} &amp; \ldots &amp; x_{2n} + y_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{m1} + y_{m1} &amp; x_{m2} + y_{m2} &amp; \ldots &amp; x_{mn} + y_{mn} \\
\end{pmatrix} 
\end{aligned}$$`

---

# Scalar addition

* `\(\mathbf{X}\)` is an `\(m \times n\)` matrix and `\(k \in \Re\)`

`$$k \mathbf{X} = \begin{pmatrix} 
k x_{11} &amp; k x_{12} &amp; \ldots &amp;  k x_{1n} \\
k x_{21} &amp; k x_{22} &amp; \ldots &amp; k x_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
k x_{m1} &amp; k x_{m2} &amp; \ldots &amp; k x_{mn} \\
\end{pmatrix}$$`

---

# Matrix transposition

`$$\mathbf{X} = \begin{pmatrix}
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1n} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{m1} &amp; x_{m2} &amp; \ldots &amp; x_{mn} \\
\end{pmatrix} \\
\mathbf{X}' = \begin{pmatrix} 
x_{11} &amp; x_{21} &amp; \ldots &amp; x_{m1} \\
x_{12} &amp; x_{22} &amp; \ldots &amp; x_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{1n} &amp; x_{2n} &amp; \ldots &amp; x_{mn}
\end{pmatrix}$$`

---

# Matrix multiplication

`$$\mathbf{X} = \begin{pmatrix} 1 &amp; 1 \\ 1&amp; 1 \\ \end{pmatrix} , \quad \mathbf{Y} = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{pmatrix}$$`

`$$\begin{aligned} \mathbf{A} &amp; = \mathbf{X} \mathbf{Y} \\&amp; = \begin{pmatrix}1 &amp; 1 \\ 1 &amp; 1 \\\end{pmatrix} \begin{pmatrix}1 &amp; 2 \\3 &amp; 4 \\\end{pmatrix} \\&amp;= \begin{pmatrix}1 \times 1 + 1 \times 3 &amp; 1 \times 2 + 1 \times 4 \\1 \times 1 + 1 \times 3 &amp; 1 \times 2 + 1 \times 4\\\end{pmatrix} \\&amp;= \begin{pmatrix}4 &amp; 6 \\4 &amp; 6\end{pmatrix}\end{aligned}$$`

---

# Algebraic properties

* Matricies must be **conformable**
* Associative property: `\((\mathbf{XY})\mathbf{Z} = \mathbf{X}(\mathbf{YZ})\)`
* Additive distributive property: `\((\mathbf{X} + \mathbf{Y})\mathbf{Z} = \mathbf{XZ} + \mathbf{YZ}\)`
* Zero property: `\(\mathbf{X0} = 0\)`
* Order matters: `\(\mathbf{XY} \neq \mathbf{YX}\)`
    * Different from scalar multiplication: `\(xy = yx\)`

---

# Neural networks

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg" style="display: block; margin: auto;" /&gt;

---

# Uses for neural networks

* Self-driving cars
* Voice activated assistants
* Automatic machine translation
* Image recognition
* Detection of diseases

---

# Linear algebra roots

* Tensor
* Scalars (0D tensors)
* Vectors (1D tensors)
* Matricies (2D tensors)
* 3D tensors and higher-dimensional tensors

---

# Tensor operations

* Generalizations of matrix operations
* Tensor addition
* Tensor multiplication
* **If you can do it with a matrix, you can do it with a tensor**

---

# Linear algebra notation

`$$\mathbf{Y} = \text{activation}(\mathbf{W} \cdot \mathbf{X} + \mathbf{B})$$`

* `\(\mathbf{X}\)`
* `\(\mathbf{Y}\)`
* `\(\mathbf{W}, \mathbf{B}\)`
* `\(\text{activation}()\)`
    * Rectified Linear Units (RELU)
        
        `$$R(z) = \max(0, z)$$`
            
    * Sigmoid function (aka logistic regression)
        
        `$$S(z) = \frac{1}{1 + e^{-z}}$$`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cfss.uchicago.edu/slides/macros.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script src="https://www.redditstatic.com/comment-embed.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
